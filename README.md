# [EMNLP 2025] Weak2Wise: An Automated, Lightweight Framework for Weak-LLM-Friendly Reasoning Synthesis

This repository provides an official reference implementation of the five-step pipeline described in Section 3 of our paper [Weak2Wise: An Automated, Lightweight Framework for Weak-LLM-Friendly Reasoning Synthesis](https://aclanthology.org/2025.findings-emnlp.1070/). It is intended to illustrate the complete **Weak2Wise** workflow in a clear and reproducible manner. Each step is implemented as a separate Python module. Configuration is centralized in `config.yaml`.

## Abstract
Recent advances in large language model (LLM) fine‐tuning have shown that training data augmented with high-quality reasoning traces can remarkably improve downstream performance. However, existing approaches usually rely on expensive manual annotations or auxiliary models, and fail to address the unique constraints of smaller “weak” LLMs. To bridge these gaps, we introduce **Weak2Wise**, a fully automated, lightweight framework for synthesizing high‐quality, weak-LLM-friendly reasoning traces. Starting from a QA dataset, Weak2Wise filters out the samples that can already be correctly answered by the weak LLM, gathers diverse candidate reasoning traces from multiple strong LLMs, and leverages our Step‐Mask scoring to rank and truncate the most guidance‐effective traces. These reasoning traces are then used for fine‐tuning, yielding substantial improvements in the weak LLM’s reasoning abilities. The name Weak2Wise has two meanings: using a “weak” LLM to select the “wisest” reasoning traces generated by stronger LLMs, and fine‐tuning the same weak LLM on these reasoning traces to become “wiser”. We further use Weak2Wise to build GR-1K, a 1,000‐sample math and science QA‐reasoning dataset optimized for weak LLMs, and fine‐tune Qwen2.5‐7B on it to create GR‐7B, which achieves superior performance on AIME2024, MATH‐500, and GPQA Diamond benchmarks.

## Project Structure

- `config.yaml`                # Centralized configuration for models, endpoints, prompts, paths, and hyperparameters
- `run_pipeline.py`            # Main entry point that executes all five steps sequentially
- `step1_filter.py`            # Step 1: Filter out samples already correctly answered by the weak LLM
- `step2_generate.py`          # Step 2: Generate diverse candidate reasoning traces using strong LLMs
- `step3_stepmask.py`          # Step 3: Score reasoning traces using the Step-Mask evaluation protocol
- `step4_select_truncate.py`   # Step 4: Select the highest-scoring trace and truncate it to the first correct derivation
- `step5_prepare_finetune.py`  # Step 5: Format selected traces into SFT-ready {"prompt", "response"} pairs
- `llm_client.py`              # Unified client supporting both OpenAI-compatible APIs and local Transformers models
- `utils.py`                   # Utility functions for I/O, text normalization, step extraction, and token counting
- `data/`
  - `qa_input.jsonl`           # Input QA dataset (user-provided)
  - `work/`                    # Intermediate outputs from each step:
      - `s_prime.jsonl`
      - `candidates.jsonl`
      - `scored_candidates.jsonl`
      - `selected_truncated.jsonl`
  - `gr1k_finetune.jsonl`      # Final fine-tuning dataset (GR-1K), ready for SFT
- `requirements.txt`           # Python dependencies

## Quick start
1. Create and activate a Python virtual environment:
   ```bash
   conda create -n weak2wise python=3.10
   conda activate weak2wise
   pip install -r requirements.txt

2. Set environment variables for API keys:
    ```bash
    export DEEPSEEK_API_KEY="sk-..."
    export DASHSCOPE_API_KEY="sk-..."

3. Update `config.yaml` if necessary (model_name, device, paths).

4. Prepare the input QA file `data/qa_input.jsonl` with lines like:
    ```json
    {"question": "What is 2+2?", "answer": "4"}

5. Run the pipeline:
    ```bash
    python run_pipeline.py

6. Outputs:

    - Intermediate files in `data/work/`:

      - `s_prime.jsonl`

      - `candidates.jsonl`

      - `scored_candidates.jsonl`

      - `selected_truncated.jsonl`

    - Final fine-tuning data: `data/gr1k_finetune.jsonl` (or `config.dataset.finetune_out`)

> Note that the pipeline involves a large number of LLM inference calls and is not optimized for efficiency. For practical use or large-scale data synthesis, we strongly recommend replacing the API calls in the code with high-throughput LLM inference services from major cloud platforms (e.g. Alibaba Cloud, etc).

## Citation

If you find our paper useful, kindly cite:

```
@inproceedings{lin-etal-2025-weak2wise,
    title = "{W}eak2{W}ise: An Automated, Lightweight Framework for Weak-{LLM}-Friendly Reasoning Synthesis",
    author = "Lin, Jianing  and
      Guo, Yuanfang  and
      Liu, Shunning  and
      Liu, Zeming  and
      Wang, Yunhong",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2025",
    month = nov,
    year = "2025",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-emnlp.1070/",
    pages = "19639--19657",
    ISBN = "979-8-89176-335-7"
}
```

